{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Example : _Classify Iris dataset_\n",
    "Fabrice Daian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\tensorflow14-keras-gpu-p35\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Python Imports needed for this scripts\n",
    "\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score,accuracy_score,log_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the data\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the training set split (80% Train / 20% Validation)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create XGBoost data structures\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set XGBoost hyperparameters\n",
    "\n",
    "num_classes = 3 # number of classes to classify\n",
    "epochs = 20     # the number of training iterations\n",
    "\n",
    "param = {\n",
    "    'max_depth': 3,  # the maximum depth of each tree\n",
    "    'eta': 0.3,  # the training step for each iteration\n",
    "    'silent': 1,  # logging mode - quiet\n",
    "    'objective': 'multi:softprob',  # error evaluation for multiclass training\n",
    "    'num_class': num_classes}  # the number of classes that exist in this datset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost training\n",
    "bst = xgb.train(param, dtrain, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the prediction on the validation dataset\n",
    "# and get the best class probability for each class\n",
    "\n",
    "preds_on_test = bst.predict(dtest)\n",
    "best_preds_on_test = np.asarray([np.argmax(line) for line in preds_on_test])\n",
    "\n",
    "preds_on_train = bst.predict(dtrain)\n",
    "best_preds_on_train = np.asarray([np.argmax(line) for line in preds_on_train])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9333333333333333\n",
      "0.3005735023257633\n"
     ]
    }
   ],
   "source": [
    "# Scoring Accuracy & loss\n",
    "\n",
    "print(accuracy_score(y_test, best_preds_on_test))\n",
    "print(log_loss(y_test, preds_on_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9313063063063063\n",
      "0.930949167791273\n",
      "0.9307242465137202\n"
     ]
    }
   ],
   "source": [
    "# Scoring Precision / Recall, F1 score\n",
    "\n",
    "print(precision_score(y_test, best_preds_on_test, average='macro'))\n",
    "print(recall_score(y_test, best_preds_on_test, average='macro'))\n",
    "print(f1_score(y_test, best_preds_on_test, average='macro'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model \n",
    "bst.dump_model('dump.raw.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
